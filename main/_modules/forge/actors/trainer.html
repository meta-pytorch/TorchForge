
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>forge.actors.trainer &#8212; torchforge 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=b61afe48" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">
import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: false,
    theme: 'base',
    themeVariables: {
        primaryColor: '#4CAF50',
        primaryTextColor: '#000',
        primaryBorderColor: '#fff',
        lineColor: '#555',
        secondaryColor: '#FF9800',
        tertiaryColor: '#ffffde'
    },
    flowchart: {
        curve: 'basis'
    },
    themeCSS: '.edgePath .path { stroke-width: 4px; stroke: #555; }'
});
</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/forge/actors/trainer';</script>
    <script src="../../../_static/custom.js?v=0065d487"></script>
    <link rel="canonical" href="https://meta-pytorch.org/torchforge/main/_modules/forge/actors/trainer.html" />
    <link rel="icon" href="../../../_static/logo-icon.svg"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "meta-pytorch/torchforge",
    github_branch: "main",
    colab_repo: "meta-pytorch/torchforge",
    colab_branch: "gh-pages"
  };
</script>

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/forge" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchforge" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchforge/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchforge" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchforge/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">forge.actors.trainer</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="forge.actors.trainer">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">_modules/forge/actors/trainer</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for forge.actors.trainer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mapping</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">fields</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed.checkpoint</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dcp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchstore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ts</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">monarch.actor</span><span class="w"> </span><span class="kn">import</span> <span class="n">endpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.checkpoint._nested_dict</span><span class="w"> </span><span class="kn">import</span> <span class="n">flatten_state_dict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtitan.config.job_config</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ActivationCheckpoint</span><span class="p">,</span>
    <span class="n">Checkpoint</span><span class="p">,</span>
    <span class="n">Comm</span><span class="p">,</span>
    <span class="n">Compile</span><span class="p">,</span>
    <span class="n">Job</span><span class="p">,</span>
    <span class="n">LRScheduler</span><span class="p">,</span>
    <span class="n">MemoryEstimation</span><span class="p">,</span>
    <span class="n">Model</span><span class="p">,</span>
    <span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">Parallelism</span><span class="p">,</span>
    <span class="n">Quantize</span><span class="p">,</span>
    <span class="n">Training</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtitan.experiments.forge.engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">ForgeEngine</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtitan.experiments.forge.job_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">ForgeJobConfig</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">forge.actors._torchstore_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">DcpHandle</span><span class="p">,</span>
    <span class="n">get_dcp_whole_state_dict_key</span><span class="p">,</span>
    <span class="n">get_param_key</span><span class="p">,</span>
    <span class="n">rdma_available</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">forge.controller</span><span class="w"> </span><span class="kn">import</span> <span class="n">ForgeActor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.data.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">batch_to_device</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.observability.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">record_metric</span><span class="p">,</span> <span class="n">Reduce</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.observability.perf_tracker</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tracer</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>


<div class="viewcode-block" id="RLTrainer">
<a class="viewcode-back" href="../../../api_trainer.html#forge.actors.trainer.RLTrainer">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">RLTrainer</span><span class="p">(</span><span class="n">ForgeActor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A reinforcement learning trainer actor for policy optimization training.</span>

<span class="sd">    Built on top of TorchTitan&#39;s training engine, this actor provides a complete training</span>
<span class="sd">    loop for reinforcement learning. It performs forward and backward passes with gradient</span>
<span class="sd">    computation, optimization steps, and checkpoint management. Unlike the ReferenceModel</span>
<span class="sd">    actor which only runs forward passes, RLTrainer actively updates the policy model</span>
<span class="sd">    parameters through gradient descent.</span>

<span class="sd">    The trainer supports the same distributed training strategies that TorchTitan does,</span>
<span class="sd">    including but not limited to, tensor parallelism, data parallelism, and FSDP</span>
<span class="sd">    (Fully Sharded Data Parallel). It is typically used in conjunction with ReferenceModel</span>
<span class="sd">    for policy optimization algorithms like GRPO (Group Relative Policy Optimization),</span>
<span class="sd">    where it optimizes the policy against a loss that includes KL divergence penalties</span>
<span class="sd">    from the reference model.</span>

<span class="sd">    The trainer handles:</span>
<span class="sd">    - Forward and backward propagation with automatic mixed precision (AMP)</span>
<span class="sd">    - Optimizer steps with learning rate scheduling</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">job</span><span class="p">:</span> <span class="n">Job</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Job</span><span class="p">)</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Model</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Model</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Optimizer</span><span class="p">)</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">LRScheduler</span><span class="p">)</span>
    <span class="n">training</span><span class="p">:</span> <span class="n">Training</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Training</span><span class="p">)</span>
    <span class="n">parallelism</span><span class="p">:</span> <span class="n">Parallelism</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Parallelism</span><span class="p">)</span>
    <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Checkpoint</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Checkpoint</span><span class="p">)</span>
    <span class="n">activation_checkpoint</span><span class="p">:</span> <span class="n">ActivationCheckpoint</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">ActivationCheckpoint</span>
    <span class="p">)</span>
    <span class="nb">compile</span><span class="p">:</span> <span class="n">Compile</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Compile</span><span class="p">)</span>
    <span class="n">quantize</span><span class="p">:</span> <span class="n">Quantize</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Quantize</span><span class="p">)</span>
    <span class="n">comm</span><span class="p">:</span> <span class="n">Comm</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Comm</span><span class="p">)</span>
    <span class="n">memory_estimation</span><span class="p">:</span> <span class="n">MemoryEstimation</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">MemoryEstimation</span><span class="p">)</span>
    <span class="c1"># Non JobConfig-related fields</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">logits</span><span class="p">,</span> <span class="o">**</span><span class="n">targets</span><span class="p">:</span> <span class="n">logits</span>
    <span class="n">state_dict_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model_state_dict&quot;</span>
    <span class="n">use_dcp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">rdma_available</span><span class="p">()</span>
    <span class="n">dcp_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;forge_dcp_tmp&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dcp</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">set_crc32_options</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">attr</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="o">**</span><span class="n">attr</span><span class="p">))</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">type</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> should be a </span><span class="si">{</span><span class="n">f</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2"> type or a dict like object&quot;</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># fragile contract.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_training_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTORCH_CUDA_ALLOC_CONF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;expandable_segments:True&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Compiling loss&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO: update ForgeEngine to not use ForgeJobConfig</span>
        <span class="n">engine_config</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">(</span><span class="bp">self</span><span class="p">)}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">{</span>
            <span class="s2">&quot;loss&quot;</span><span class="p">,</span>
            <span class="s2">&quot;state_dict_key&quot;</span><span class="p">,</span>
            <span class="s2">&quot;use_dcp&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dcp_path&quot;</span><span class="p">,</span>
        <span class="p">}:</span>
            <span class="n">engine_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>  <span class="c1"># Not part of job config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">ForgeEngine</span><span class="p">(</span><span class="n">ForgeJobConfig</span><span class="p">(</span><span class="o">**</span><span class="n">engine_config</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<div class="viewcode-block" id="RLTrainer.forward_backward">
<a class="viewcode-back" href="../../../api_trainer.html#forge.actors.trainer.RLTrainer.forward_backward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward_backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">targets</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">model_parts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">model_parts</span>
        <span class="n">parallel_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">parallel_dims</span>
        <span class="n">optional_context_parallel_ctx</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">parallel_dims</span><span class="o">.</span><span class="n">pp_enabled</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;PP not implemented yet&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">train_context</span><span class="p">(</span><span class="n">optional_context_parallel_ctx</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_parts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">maybe_enable_amp</span><span class="p">:</span>
                    <span class="n">logits</span> <span class="o">=</span> <span class="n">model_parts</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">**</span><span class="n">targets</span><span class="p">)</span>
                <span class="k">del</span> <span class="n">logits</span>  <span class="c1"># Free to before bwd to avoid peaking memory</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div>


    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]],</span> <span class="n">targets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;rl_trainer_perf/step&quot;</span><span class="p">,</span> <span class="n">timer</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">track_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">gc_handler</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
        <span class="n">local_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">dp_rank</span><span class="p">]</span>
        <span class="n">local_targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">dp_rank</span><span class="p">]</span>
        <span class="n">batch_to_device</span><span class="p">(</span><span class="n">local_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch_to_device</span><span class="p">(</span><span class="n">local_targets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_backward</span><span class="p">(</span><span class="n">local_inputs</span><span class="p">,</span> <span class="n">local_targets</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;forward_backward&quot;</span><span class="p">)</span>

        <span class="n">current_lr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="p">,</span> <span class="s2">&quot;get_last_lr&quot;</span><span class="p">)</span>
            <span class="k">else</span> <span class="mf">0.001</span>
        <span class="p">)</span>
        <span class="n">record_metric</span><span class="p">(</span><span class="s2">&quot;rl_trainer/learning_rate&quot;</span><span class="p">,</span> <span class="n">current_lr</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">MIN</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;optimizer_step&quot;</span><span class="p">)</span>

        <span class="c1"># TODO: delete item() to avoid cpu-gpu sync</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">record_metric</span><span class="p">(</span><span class="s2">&quot;rl_trainer/count_training_steps&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="n">record_metric</span><span class="p">(</span><span class="s2">&quot;rl_trainer/avg_grpo_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span>

        <span class="c1"># These are placeholder values until the loss function exposes these metrics</span>
        <span class="c1"># record_metric(&quot;rl_trainer/step/avg_kl_divergence&quot;, 0.0, Reduce.MEAN)</span>
        <span class="c1"># record_metric(&quot;rl_trainer/step/std_kl_divergence&quot;, 0.0, Reduce.STD)</span>
        <span class="c1"># record_metric(&quot;rl_trainer/step/avg_policy_entropy&quot;, 0.0, Reduce.MEAN)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="n">curr_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
            <span class="n">last_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_training_steps</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;save_checkpoint&quot;</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">push_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_version</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Push weights to torchstore in HF format.&quot;&quot;&quot;</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;rl_trainer_perf/push_weights&quot;</span><span class="p">,</span> <span class="n">timer</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">track_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pushing weights for policy version </span><span class="si">{</span><span class="n">policy_version</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;model&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model state not found in checkpointer state&quot;</span><span class="p">)</span>

        <span class="n">sd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">flattened_state_dict</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">flatten_state_dict</span><span class="p">(</span><span class="n">sd</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;flatten_state_dict&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">sd_adapter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Trying to save checkpoint in HF safetensors format, but sd_adapter is not provided.&quot;</span>
            <span class="p">)</span>
        <span class="n">hf_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">sd_adapter</span><span class="o">.</span><span class="n">to_hf</span><span class="p">(</span><span class="n">flattened_state_dict</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;to_hf&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dcp</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">get_dcp_whole_state_dict_key</span><span class="p">(</span><span class="n">policy_version</span><span class="p">)</span>
            <span class="n">dcp_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dcp_path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">storage_writer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">FileSystemWriter</span><span class="p">(</span>
                <span class="n">dcp_id</span><span class="p">,</span> <span class="n">single_file_per_rank</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">thread_count</span><span class="o">=</span><span class="mi">8</span>
            <span class="p">)</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">dcp</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">storage_writer</span><span class="o">=</span><span class="n">storage_writer</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">hf_state_dict</span><span class="p">)</span>
            <span class="n">dcp_handle</span> <span class="o">=</span> <span class="n">DcpHandle</span><span class="p">(</span>
                <span class="n">checkpoint_id</span><span class="o">=</span><span class="n">dcp_id</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">param_names</span><span class="o">=</span><span class="n">hf_state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">dcp_handle</span><span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;dcp_save&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">hf_state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">key</span> <span class="o">=</span> <span class="n">get_param_key</span><span class="p">(</span><span class="n">policy_version</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;ts_save&quot;</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Completed weights push in </span><span class="si">%.2f</span><span class="s2"> seconds&quot;</span><span class="p">,</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">cleanup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div>

</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "forge.actors.trainer",
       "headline": "forge.actors.trainer",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/forge/actors/trainer.html",
       "articleBody": "Source code for forge.actors.trainer # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD-style license found in the # LICENSE file in the root directory of this source tree. import logging import os import time from collections.abc import Mapping from dataclasses import dataclass, field, fields from typing import Callable import torch import torch.distributed.checkpoint as dcp import torchstore as ts from monarch.actor import endpoint from torch import Tensor from torch.distributed.checkpoint._nested_dict import flatten_state_dict from torchtitan.config.job_config import ( ActivationCheckpoint, Checkpoint, Comm, Compile, Job, LRScheduler, MemoryEstimation, Model, Optimizer, Parallelism, Quantize, Training, ) from torchtitan.experiments.forge.engine import ForgeEngine from torchtitan.experiments.forge.job_config import ForgeJobConfig from forge.actors._torchstore_utils import ( DcpHandle, get_dcp_whole_state_dict_key, get_param_key, rdma_available, ) from forge.controller import ForgeActor from forge.data.utils import batch_to_device from forge.observability.metrics import record_metric, Reduce from forge.observability.perf_tracker import Tracer logger = logging.getLogger(__name__) logger.setLevel(logging.DEBUG) [docs] @dataclass class RLTrainer(ForgeActor): \"\"\"A reinforcement learning trainer actor for policy optimization training. Built on top of TorchTitan\u0027s training engine, this actor provides a complete training loop for reinforcement learning. It performs forward and backward passes with gradient computation, optimization steps, and checkpoint management. Unlike the ReferenceModel actor which only runs forward passes, RLTrainer actively updates the policy model parameters through gradient descent. The trainer supports the same distributed training strategies that TorchTitan does, including but not limited to, tensor parallelism, data parallelism, and FSDP (Fully Sharded Data Parallel). It is typically used in conjunction with ReferenceModel for policy optimization algorithms like GRPO (Group Relative Policy Optimization), where it optimizes the policy against a loss that includes KL divergence penalties from the reference model. The trainer handles: - Forward and backward propagation with automatic mixed precision (AMP) - Optimizer steps with learning rate scheduling \"\"\" job: Job = field(default_factory=Job) model: Model = field(default_factory=Model) optimizer: Optimizer = field(default_factory=Optimizer) lr_scheduler: LRScheduler = field(default_factory=LRScheduler) training: Training = field(default_factory=Training) parallelism: Parallelism = field(default_factory=Parallelism) checkpoint: Checkpoint = field(default_factory=Checkpoint) activation_checkpoint: ActivationCheckpoint = field( default_factory=ActivationCheckpoint ) compile: Compile = field(default_factory=Compile) quantize: Quantize = field(default_factory=Quantize) comm: Comm = field(default_factory=Comm) memory_estimation: MemoryEstimation = field(default_factory=MemoryEstimation) # Non JobConfig-related fields loss: Callable = lambda logits, **targets: logits state_dict_key: str = \"model_state_dict\" use_dcp: bool = not rdma_available() dcp_path: str = \"forge_dcp_tmp\" def __post_init__(self): super().__init__() if self.use_dcp: torch.serialization.set_crc32_options(False) for f in fields(self): attr = getattr(self, f.name) if isinstance(attr, Mapping): setattr(self, f.name, f.type(**attr)) elif not isinstance(attr, f.type): raise TypeError( f\"{f.name} should be a {f.type} type or a dict like object\" ) self.step = 1 # fragile contract. self.num_training_steps = self.training.steps self.gradient_accumulation_steps = 1 os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" logger.info(\"Compiling loss\") self.loss = torch.compile(self.loss) @endpoint async def setup(self): # TODO: update ForgeEngine to not use ForgeJobConfig engine_config = {f.name: getattr(self, f.name) for f in fields(self)} for key in { \"loss\", \"state_dict_key\", \"use_dcp\", \"dcp_path\", }: engine_config.pop(key) # Not part of job config self.engine = ForgeEngine(ForgeJobConfig(**engine_config)) self.engine.checkpointer.load(step=self.step) self.engine.optimizers.zero_grad() [docs] def forward_backward( self, inputs: dict[str, Tensor], targets: dict[str, Tensor] ) -\u003e Tensor: model_parts = self.engine.model_parts parallel_dims = self.engine.parallel_dims optional_context_parallel_ctx = None if parallel_dims.pp_enabled: raise NotImplementedError(\"PP not implemented yet\") else: with self.engine.train_context(optional_context_parallel_ctx): assert len(model_parts) == 1 with self.engine.maybe_enable_amp: logits = model_parts[0](**inputs) loss = self.loss(logits, **targets) del logits # Free to before bwd to avoid peaking memory loss.backward() return loss @endpoint async def train_step( self, inputs: list[dict[str, Tensor]], targets: list[dict[str, Tensor]] ) -\u003e float: t = Tracer(\"rl_trainer_perf/step\", timer=\"gpu\", track_memory=True) t.start() self.engine.gc_handler.run(self.step) local_inputs = inputs[self.engine.dp_rank] local_targets = targets[self.engine.dp_rank] batch_to_device(local_inputs, self.engine.device) batch_to_device(local_targets, self.engine.device) loss = self.forward_backward(local_inputs, local_targets) torch.distributed.all_reduce(loss) t.step(\"forward_backward\") current_lr = ( self.engine.lr_schedulers.get_last_lr()[0] if hasattr(self.engine.lr_schedulers, \"get_last_lr\") else 0.001 ) record_metric(\"rl_trainer/learning_rate\", current_lr, Reduce.MIN) self.engine.optimizers.step() self.engine.optimizers.zero_grad() self.engine.lr_schedulers.step() t.step(\"optimizer_step\") # TODO: delete item() to avoid cpu-gpu sync loss = loss.detach().item() record_metric(\"rl_trainer/count_training_steps\", 1, Reduce.SUM) record_metric(\"rl_trainer/avg_grpo_loss\", loss, Reduce.MEAN) # These are placeholder values until the loss function exposes these metrics # record_metric(\"rl_trainer/step/avg_kl_divergence\", 0.0, Reduce.MEAN) # record_metric(\"rl_trainer/step/std_kl_divergence\", 0.0, Reduce.STD) # record_metric(\"rl_trainer/step/avg_policy_entropy\", 0.0, Reduce.MEAN) self.step += 1 self.engine.checkpointer.save( curr_step=self.step, last_step=self.step == self.num_training_steps, ) t.step(\"save_checkpoint\") t.stop() return loss @endpoint async def push_weights(self, policy_version: int) -\u003e None: \"\"\"Push weights to torchstore in HF format.\"\"\" t = Tracer(\"rl_trainer_perf/push_weights\", timer=\"gpu\", track_memory=True) t.start() logger.info(f\"Pushing weights for policy version {policy_version}\") start_time = time.perf_counter() if \"model\" not in self.engine.checkpointer.states: raise RuntimeError(\"Model state not found in checkpointer state\") sd = self.engine.checkpointer.states[\"model\"].state_dict() flattened_state_dict, _ = flatten_state_dict(sd) t.step(\"flatten_state_dict\") if self.engine.checkpointer.sd_adapter is None: raise RuntimeError( \"Trying to save checkpoint in HF safetensors format, but sd_adapter is not provided.\" ) hf_state_dict = self.engine.checkpointer.sd_adapter.to_hf(flattened_state_dict) t.step(\"to_hf\") if self.use_dcp: key = get_dcp_whole_state_dict_key(policy_version) dcp_id = f\"{self.dcp_path}/{key}\" storage_writer = torch.distributed.checkpoint.FileSystemWriter( dcp_id, single_file_per_rank=False, thread_count=8 ) metadata = dcp.save(storage_writer=storage_writer, state_dict=hf_state_dict) dcp_handle = DcpHandle( checkpoint_id=dcp_id, metadata=metadata, param_names=hf_state_dict.keys(), ) await ts.put(key, dcp_handle) t.step(\"dcp_save\") else: for name, param in hf_state_dict.items(): key = get_param_key(policy_version, name) await ts.put(key, param) t.step(\"ts_save\") t.stop() end_time = time.perf_counter() logger.info(\"Completed weights push in %.2f seconds\", end_time - start_time) @endpoint async def cleanup(self) -\u003e None: if self.engine.checkpointer: self.engine.checkpointer.close()",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/forge/actors/trainer.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>