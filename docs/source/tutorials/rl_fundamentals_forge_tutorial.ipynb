{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# RL Fundamentals Using Forge Terminology\n\n**Author:** [Your Name](https://github.com/yourusername)\n\n.. grid:: 2\n\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\n       :class-card: card-prerequisites\n\n       * Core RL components in Forge (Dataset, Policy, Reward Model, etc.)\n       * How RL concepts map to distributed Forge services\n       * Building scalable RL training loops with fault tolerance\n       * Resource management and independent scaling patterns\n\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\n       :class-card: card-prerequisites\n\n       * PyTorch v2.0.0+\n       * GPU access recommended\n       * Basic understanding of reinforcement learning\n       * Familiarity with async/await in Python\n\nThis tutorial teaches RL fundamentals using Forge's exact terminology and architecture.\nWe'll start with a simple math tutoring example to understand how traditional RL concepts\nmap to Forge's distributed service model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core RL Components in Forge\n\nLet's start with a simple math tutoring example to understand RL concepts\nwith the exact names Forge uses. Think of it as teaching an AI student:\n\n- **Dataset**: Provides questions (like \"What is 2+2?\")\n- **Policy**: The AI student being trained (generates answers like \"The answer is 4\")\n- **Reward Model**: The teacher that evaluates answer quality (gives scores like 0.95)\n- **Reference Model**: Copy of original student (prevents drift from baseline)\n- **Replay Buffer**: Notebook that stores experiences (question + answer + score)\n- **Trainer**: The tutor that improves the student based on experiences\n\nHere's how these components interact in a conceptual RL step:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nfrom typing import Any, Dict, Optional\n\nimport torch\n\n\ndef conceptual_rl_step():\n    \"\"\"\n    Conceptual example showing the RL learning flow.\n    See apps/grpo/main.py for actual GRPO implementation.\n    \"\"\"\n    # 1. Get a math problem\n    question = \"What is 2+2?\"  # dataset.sample()\n\n    # 2. Student generates answer\n    answer = \"The answer is 4\"  # policy.generate(question)\n\n    # 3. Teacher grades it\n    score = 0.95  # reward_model.evaluate(question, answer)\n\n    # 4. Compare to original student\n    baseline = 0.85  # reference_model.compute_logprobs(question, answer)\n\n    # 5. Store the experience\n    experience = {\n        \"question\": question,\n        \"answer\": answer,\n        \"score\": score,\n        \"baseline\": baseline,\n    }\n    # replay_buffer.add(experience)\n\n    # 6. When enough experiences collected, improve student\n    # trainer.train_step(batch)  # Student gets better!\n\n    return experience\n\n\nexample_experience = conceptual_rl_step()\nprint(\"Example RL experience:\", example_experience)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From Concepts to Forge Services\n\nHere's the key insight: **Each RL component becomes a Forge service**.\nThe toy example above maps directly to Forge's distributed architecture:\n\n* Dataset \u2192 DatasetActor\n* Policy \u2192 Policy\n* Reward Model \u2192 RewardActor\n* Reference Model \u2192 ReferenceModel\n* Replay Buffer \u2192 ReplayBuffer\n* Trainer \u2192 RLTrainer\n\nLet's see how the conceptual example translates to actual Forge service calls:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def forge_rl_step(services: Dict[str, Any], step: int) -> Optional[float]:\n    \"\"\"\n    RL step using actual Forge service APIs.\n    This shows the same logic as conceptual_rl_step but with real service calls.\n    \"\"\"\n    # 1. Get a math problem - Using actual DatasetActor API\n    sample = await services[\"dataloader\"].sample.call_one()\n    prompt, target = sample[\"request\"], sample[\"target\"]\n\n    # 2. Student generates answer - Using actual Policy API\n    responses = await services[\"policy\"].generate.route(prompt=prompt)\n    answer = responses[0].text\n\n    # 3. Teacher grades it - Using actual RewardActor API\n    score = await services[\"reward_actor\"].evaluate_response.route(\n        prompt=prompt, response=answer, target=target\n    )\n\n    # 4. Compare to baseline - Using actual ReferenceModel API\n    # Note: ReferenceModel.forward requires input_ids, max_req_tokens, return_logprobs\n    input_ids = torch.cat([responses[0].prompt_ids, responses[0].token_ids])\n    ref_logprobs = await services[\"ref_model\"].forward.route(\n        input_ids.unsqueeze(0), max_req_tokens=512, return_logprobs=True\n    )\n\n    # 5. Store experience - Using actual Episode structure from apps/grpo/main.py\n    episode = create_episode_from_response(responses[0], score, ref_logprobs, step)\n    await services[\"replay_buffer\"].add.call_one(episode)\n\n    # 6. Improve student - Using actual trainer pattern\n    batch = await services[\"replay_buffer\"].sample.call_one(curr_policy_version=step)\n    if batch is not None:\n        inputs, targets = batch  # GRPO returns (inputs, targets) tuple\n        loss = await services[\"trainer\"].train_step.call(inputs, targets)\n\n        # 7. Policy synchronization - Using actual weight update pattern\n        await services[\"trainer\"].push_weights.call(step + 1)\n        await services[\"policy\"].update_weights.fanout(step + 1)\n\n        return loss\n\n    return None\n\n\ndef create_episode_from_response(response, score, ref_logprobs, step):\n    \"\"\"Helper function to create episode from response data\"\"\"\n    return {\n        \"response\": response,\n        \"score\": score,\n        \"ref_logprobs\": ref_logprobs,\n        \"step\": step,\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Forge Services\n\nHere's how to initialize the complete RL system with proper resource allocation.\nEach service can scale independently based on its computational needs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def setup_forge_rl_system():\n    \"\"\"\n    Complete setup of Forge RL services with proper resource allocation.\n    This example uses Qwen 3.1-1.7B model for demonstration.\n    \"\"\"\n    # Note: In actual Forge environment, imports would be:\n    # from forge.actors.policy import Policy\n    # from forge.actors.replay_buffer import ReplayBuffer\n    # from forge.actors.reference_model import ReferenceModel\n    # from forge.actors.trainer import RLTrainer\n    # from apps.grpo.main import DatasetActor, RewardActor, ComputeAdvantages\n    # from forge.data.rewards import MathReward, ThinkingReward\n\n    model = \"Qwen/Qwen3-1.7B\"\n    group_size = 1\n\n    # Initialize all services with appropriate resource allocation\n    services = await asyncio.gather(\n        # Dataset actor (CPU intensive for I/O)\n        create_dataset_actor(model),\n        # Policy service (GPU for inference)\n        create_policy_service(model, group_size),\n        # Trainer actor (GPU for training)\n        create_trainer_actor(model),\n        # Replay buffer (CPU for memory management)\n        create_replay_buffer_actor(),\n        # Advantage computation (CPU)\n        create_advantages_actor(),\n        # Reference model (GPU for baseline)\n        create_reference_model_actor(model),\n        # Reward actor (CPU/small GPU for evaluation)\n        create_reward_actor(),\n    )\n\n    service_names = [\n        \"dataloader\",\n        \"policy\",\n        \"trainer\",\n        \"replay_buffer\",\n        \"compute_advantages\",\n        \"ref_model\",\n        \"reward_actor\",\n    ]\n\n    return dict(zip(service_names, services))\n\n\n# Service creation functions (would use actual Forge APIs)\nasync def create_dataset_actor(model):\n    \"\"\"DatasetActor for loading training data\"\"\"\n    return {\n        \"name\": \"DatasetActor\",\n        \"config\": {\n            \"path\": \"openai/gsm8k\",\n            \"revision\": \"main\",\n            \"data_split\": \"train\",\n            \"streaming\": True,\n            \"model\": model,\n        },\n        \"resources\": \"CPU\",\n        \"sample\": lambda: {\n            \"call_one\": lambda: {\"request\": \"What is 2+2?\", \"target\": \"4\"}\n        },\n    }\n\n\nasync def create_policy_service(model, group_size):\n    \"\"\"Policy service for text generation\"\"\"\n    return {\n        \"name\": \"Policy\",\n        \"config\": {\n            \"engine_config\": {\n                \"model\": model,\n                \"tensor_parallel_size\": 1,\n                \"pipeline_parallel_size\": 1,\n                \"enforce_eager\": False,\n            },\n            \"sampling_config\": {\n                \"n\": group_size,\n                \"max_tokens\": 16,\n                \"temperature\": 1.0,\n                \"top_p\": 1.0,\n            },\n        },\n        \"resources\": \"GPU\",\n        \"generate\": lambda: {\"route\": lambda prompt: [MockResponse()]},\n    }\n\n\nasync def create_trainer_actor(model):\n    \"\"\"RLTrainer for policy optimization\"\"\"\n    return {\n        \"name\": \"RLTrainer\",\n        \"config\": {\n            \"model\": {\n                \"name\": \"qwen3\",\n                \"flavor\": \"1.7B\",\n                \"hf_assets_path\": f\"hf://{model}\",\n            },\n            \"optimizer\": {\"name\": \"AdamW\", \"lr\": 1e-5},\n            \"training\": {\"local_batch_size\": 2, \"seq_len\": 2048},\n        },\n        \"resources\": \"GPU\",\n        \"train_step\": lambda: {\"call\": lambda inputs, targets: 0.5},\n    }\n\n\nasync def create_replay_buffer_actor():\n    \"\"\"ReplayBuffer for experience storage\"\"\"\n    return {\n        \"name\": \"ReplayBuffer\",\n        \"config\": {\"batch_size\": 2, \"max_policy_age\": 1, \"dp_size\": 1},\n        \"resources\": \"CPU\",\n        \"add\": lambda: {\"call_one\": lambda episode: None},\n        \"sample\": lambda: {\"call_one\": lambda curr_policy_version: ([], [])},\n    }\n\n\nasync def create_advantages_actor():\n    \"\"\"ComputeAdvantages for advantage estimation\"\"\"\n    return {\"name\": \"ComputeAdvantages\", \"resources\": \"CPU\"}\n\n\nasync def create_reference_model_actor(model):\n    \"\"\"ReferenceModel for baseline computation\"\"\"\n    return {\n        \"name\": \"ReferenceModel\",\n        \"config\": {\n            \"model\": {\n                \"name\": \"qwen3\",\n                \"flavor\": \"1.7B\",\n                \"hf_assets_path\": f\"hf://{model}\",\n            },\n            \"training\": {\"dtype\": \"bfloat16\"},\n        },\n        \"resources\": \"GPU\",\n        \"forward\": lambda: {\n            \"route\": lambda input_ids, max_req_tokens, return_logprobs: torch.tensor(\n                [0.1, 0.2]\n            )\n        },\n    }\n\n\nasync def create_reward_actor():\n    \"\"\"RewardActor for response evaluation\"\"\"\n    return {\n        \"name\": \"RewardActor\",\n        \"config\": {\"reward_functions\": [\"MathReward\", \"ThinkingReward\"]},\n        \"resources\": \"CPU\",\n        \"evaluate_response\": lambda: {\"route\": lambda prompt, response, target: 0.95},\n    }\n\n\nclass MockResponse:\n    \"\"\"Mock response object for demonstration\"\"\"\n\n    def __init__(self):\n        self.text = \"The answer is 4\"\n        self.prompt_ids = torch.tensor([1, 2, 3])\n        self.token_ids = torch.tensor([4, 5, 6])\n\n\n# Demonstrate the setup\nprint(\"Setting up Forge RL system...\")\n# services = await setup_forge_rl_system()  # Would work in async context\nprint(\"Forge services configured with independent scaling capabilities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Forge Architecture Matters\n\nTraditional ML infrastructure fails for RL because each component has\ndifferent resource needs, scaling patterns, and failure modes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show_infrastructure_challenges():\n    \"\"\"\n    Demonstrate why traditional monolithic RL fails and how Forge solves it.\n    \"\"\"\n    print(\"=== Infrastructure Challenges ===\\n\")\n\n    print(\"Problem 1: Different Resource Needs\")\n    resource_requirements = {\n        \"Policy (Student AI)\": {\n            \"generates\": \"'The answer is 4'\",\n            \"needs\": \"Large GPU memory\",\n            \"scaling\": \"Multiple replicas for speed\",\n        },\n        \"Reward Model (Teacher)\": {\n            \"scores\": \"answers: 0.95\",\n            \"needs\": \"Moderate compute\",\n            \"scaling\": \"CPU or small GPU\",\n        },\n        \"Trainer (Tutor)\": {\n            \"improves\": \"student weights\",\n            \"needs\": \"Massive GPU compute\",\n            \"scaling\": \"Distributed training\",\n        },\n        \"Dataset (Question Bank)\": {\n            \"provides\": \"'What is 2+2?'\",\n            \"needs\": \"CPU intensive I/O\",\n            \"scaling\": \"High memory bandwidth\",\n        },\n    }\n\n    for component, reqs in resource_requirements.items():\n        print(f\"{component}:\")\n        for key, value in reqs.items():\n            print(f\"  {key}: {value}\")\n        print()\n\n    print(\"Problem 2: Coordination Complexity\")\n    print(\"Unlike supervised learning with independent batches,\")\n    print(\"RL requires complex coordination between components:\")\n    print(\"- Policy waits idle while reward model works\")\n    print(\"- Training waits for single episode (batch size = 1)\")\n    print(\"- Everything stops if any component fails\")\n    print()\n\n    print(\"=== Forge Solutions ===\\n\")\n\n    print(\"\u2705 Automatic Resource Management\")\n    print(\"- Routing to least loaded replica\")\n    print(\"- GPU memory management\")\n    print(\"- Batch optimization\")\n    print(\"- Failure recovery\")\n    print(\"- Auto-scaling based on demand\")\n    print()\n\n    print(\"\u2705 Independent Scaling\")\n    print(\"- Policy: num_replicas=8 for high inference demand\")\n    print(\"- RewardActor: num_replicas=16 for parallel evaluation\")\n    print(\"- Trainer: Multiple actors for distributed training\")\n    print()\n\n    print(\"\u2705 Fault Tolerance\")\n    print(\"- Automatic routing to healthy replicas\")\n    print(\"- Background replica respawn\")\n    print(\"- Graceful degradation\")\n    print(\"- System continues during component failures\")\n\n\nshow_infrastructure_challenges()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Scaling Example\n\nHere's how you would scale the system for production workloads:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def demonstrate_production_scaling():\n    \"\"\"\n    Show how Forge services scale independently for production.\n    \"\"\"\n    print(\"=== Production Scaling Configuration ===\\n\")\n\n    scaling_config = {\n        \"Policy Service\": {\n            \"replicas\": 8,\n            \"reason\": \"High inference demand from multiple training runs\",\n            \"resources\": \"GPU-heavy instances\",\n        },\n        \"RewardActor Service\": {\n            \"replicas\": 16,\n            \"reason\": \"Parallel evaluation of many responses\",\n            \"resources\": \"CPU/small GPU instances\",\n        },\n        \"Trainer Actor\": {\n            \"replicas\": 4,\n            \"reason\": \"Distributed training across multiple nodes\",\n            \"resources\": \"Large GPU clusters\",\n        },\n        \"Dataset Actor\": {\n            \"replicas\": 2,\n            \"reason\": \"I/O intensive data loading\",\n            \"resources\": \"High-bandwidth CPU instances\",\n        },\n        \"ReplayBuffer Actor\": {\n            \"replicas\": 1,\n            \"reason\": \"Centralized experience storage\",\n            \"resources\": \"High-memory instances\",\n        },\n    }\n\n    for service, config in scaling_config.items():\n        print(f\"{service}:\")\n        print(f\"  Replicas: {config['replicas']}\")\n        print(f\"  Reason: {config['reason']}\")\n        print(f\"  Resources: {config['resources']}\")\n        print()\n\n    print(\"Key Benefits:\")\n    print(\"- Each service scales based on its bottlenecks\")\n    print(\"- Resource utilization is optimized\")\n    print(\"- Costs are minimized (no idle GPUs)\")\n    print(\"- System maintains performance under load\")\n\n\ndemonstrate_production_scaling()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete RL Training Loop\n\nHere's a complete example showing multiple RL training steps:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def complete_rl_training_example(num_steps: int = 5):\n    \"\"\"\n    Complete RL training loop using Forge services.\n    \"\"\"\n    print(f\"=== Running {num_steps} RL Training Steps ===\\n\")\n\n    # Setup services (mock for demonstration)\n    services = {\n        \"dataloader\": await create_dataset_actor(\"Qwen/Qwen3-1.7B\"),\n        \"policy\": await create_policy_service(\"Qwen/Qwen3-1.7B\", 1),\n        \"trainer\": await create_trainer_actor(\"Qwen/Qwen3-1.7B\"),\n        \"replay_buffer\": await create_replay_buffer_actor(),\n        \"ref_model\": await create_reference_model_actor(\"Qwen/Qwen3-1.7B\"),\n        \"reward_actor\": await create_reward_actor(),\n    }\n\n    losses = []\n\n    for step in range(num_steps):\n        print(f\"Step {step + 1}:\")\n\n        # Simulate the RL step (would use actual forge_rl_step in practice)\n        sample = await services[\"dataloader\"][\"sample\"]()[\"call_one\"]()\n        print(f\"  Question: {sample['request']}\")\n        print(f\"  Target: {sample['target']}\")\n\n        # Generate response\n        responses = await services[\"policy\"][\"generate\"]()[\"route\"](sample[\"request\"])\n        print(f\"  Generated: {responses[0].text}\")\n\n        # Get reward\n        score = await services[\"reward_actor\"][\"evaluate_response\"]()[\"route\"](\n            sample[\"request\"], responses[0].text, sample[\"target\"]\n        )\n        print(f\"  Reward: {score}\")\n\n        # Simulate training (every few steps when buffer has enough data)\n        if step >= 2:  # Start training after accumulating some experience\n            loss = await services[\"trainer\"][\"train_step\"]()[\"call\"]([], [])\n            losses.append(loss)\n            print(f\"  Training Loss: {loss:.4f}\")\n\n        print()\n\n    print(f\"Training completed! Average loss: {sum(losses)/len(losses):.4f}\")\n    return losses\n\n\n# Run the example (would work in async context)\nprint(\"Complete RL training example:\")\nprint(\"(In real usage, run: await complete_rl_training_example(5))\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nThis tutorial demonstrated how RL fundamentals map to Forge's distributed\nservice architecture. Key takeaways:\n\n1. **Service Mapping**: Each RL component (Dataset, Policy, Reward, etc.)\n   becomes an independent, scalable Forge service\n\n2. **Resource Optimization**: Services scale independently based on their\n   computational needs (GPU for inference/training, CPU for data/rewards)\n\n3. **Fault Tolerance**: Individual service failures don't stop the entire\n   training pipeline - Forge handles routing and recovery automatically\n\n4. **Simple Interface**: Complex distributed systems are hidden behind\n   simple async function calls\n\nThe same RL logic that works conceptually scales to production workloads\nwithout infrastructure code - Forge handles distribution, scaling, and\nfault tolerance automatically.\n\n## Further Reading\n\n* [Forge Architecture Documentation](#)\n* [GRPO Implementation (apps/grpo/main.py)](#)\n* [Forge Service APIs](#)\n* [Production RL Scaling Guide](#)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}