{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Supervised Fine-Tuning (SFT) with Forge\n",
        "\n",
        "Welcome! This notebook will guide you through fine-tuning large language models using **Forge**, a distributed training framework powered by PyTorch.\n",
        "\n",
        "## What You'll Do\n",
        "\n",
        "In just a few steps, you'll:\n",
        "1. Configure your model and training parameters\n",
        "2. Run distributed training across multiple GPUs\n",
        "3. Save checkpoints and monitor progress\n",
        "\n",
        "**No need to worry about distributed training complexity** - Forge handles multi-GPU/multi-node coordination automatically!\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Start (TL;DR)\n",
        "\n",
        "For the impatient, here's all you need:\n",
        "\n",
        "```python\n",
        "# 1. Import\n",
        "from apps.sft.trainer_actor import TrainerActor\n",
        "from apps.sft.spawn_actor import run_actor\n",
        "\n",
        "# 2. Configure (edit paths and hyperparameters)\n",
        "cfg = OmegaConf.create({...})  # See configuration section below\n",
        "\n",
        "# 3. Run (that's it!)\n",
        "await run_actor(TrainerActor, cfg)\n",
        "```\n",
        "\n",
        "**What happens under the hood:**\n",
        "- Forge automatically spawns processes across your GPUs\n",
        "- Model is sharded using FSDP (Fully Sharded Data Parallel)\n",
        "- Training runs for the specified number of steps\n",
        "- Checkpoints are saved periodically\n",
        "\n",
        "Let's dive into the details...\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Import Dependencies\n",
        "\n",
        "Let's import the tools we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "\n",
        "from apps.sft.trainer_actor import TrainerActor\n",
        "from apps.sft.spawn_actor import run_actor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Configuration\n",
        "\n",
        "## Overview\n",
        "\n",
        "Training configuration has several sections. Don't worry - most defaults work well!\n",
        "\n",
        "**What you MUST change:**\n",
        "- Model paths (`hf_assets_path`, checkpoint `folder`)\n",
        "- Dataset name\n",
        "- Number of GPUs (`procs`)\n",
        "\n",
        "**What you MIGHT want to tune:**\n",
        "- Training steps and batch size\n",
        "- Learning rate\n",
        "- Checkpoint save frequency\n",
        "\n",
        "Let's go through each section:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base configuration from YAML file\n",
        "base_config_path = \"Change to the path\"  # Change this to use different base config\n",
        "\n",
        "cfg = OmegaConf.load(base_config_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Model and GPU Settings\n",
        "\n",
        "**Critical settings to update:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "# ‚ö†Ô∏è CHANGE THIS: Update the path to your model\n",
        "model_config = {\n",
        "    \"name\": \"llama3\",\n",
        "    \"flavor\": \"8B\",\n",
        "    \"hf_assets_path\": \"/path/to/your/hf\"  # ‚Üê UPDATE THIS PATH\n",
        "}\n",
        "\n",
        "# Process Configuration\n",
        "# ‚ö†Ô∏è CHANGE THIS: Set to number of GPUs you have\n",
        "processes_config = {\n",
        "    \"procs\": 8,        # ‚Üê UPDATE THIS (e.g., 1, 4, 8)\n",
        "    \"with_gpus\": True\n",
        "}\n",
        "\n",
        "\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(model_config)))\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(processes_config)))\n",
        "# Adding to the yaml file\n",
        "cfg.model = OmegaConf.create(model_config)\n",
        "cfg.processes = OmegaConf.create(processes_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer and Learning Rate\n",
        "\n",
        "**Good defaults** - usually you won't need to change these:\n",
        "\n",
        "- **AdamW**: Best optimizer for transformers\n",
        "- **Learning rate (1e-5)**: Safe starting point for fine-tuning\n",
        "- **Warmup steps**: Gradually increases LR to prevent instability\n",
        "\n",
        "**When to adjust:**\n",
        "- If loss doesn't decrease ‚Üí Try 2e-5 or 5e-5\n",
        "- If loss becomes NaN ‚Üí Lower LR to 5e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Training Settings\n",
        "\n",
        "### Core Training Parameters\n",
        "\n",
        "**local_batch_size**: Examples processed per GPU per step\n",
        "- Start with 1 for large models (8B+)\n",
        "- Increase to 2-4 if you have memory headroom\n",
        "- Global batch = local_batch_size √ó num_GPUs\n",
        "\n",
        "**seq_len**: Maximum sequence length in tokens\n",
        "- 2048 tokens ‚âà 1500 words\n",
        "- Longer sequences = more context but slower training\n",
        "- Reduce if running out of memory\n",
        "\n",
        "**steps**: Total number of training iterations\n",
        "- 100-500: Quick experiment\n",
        "- 1000-5000: Solid fine-tune\n",
        "- 10000+: Production training\n",
        "\n",
        "**dataset**: Training data source (e.g., \"c4\", \"alpaca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è CHANGE dataset to match your data\n",
        "training_config = {\n",
        "    \"local_batch_size\": 1,  # Increase to 2-4 if you have memory\n",
        "    \"seq_len\": 2048,        # Sequence length\n",
        "    \"max_norm\": 1.0,        # Gradient clipping (prevents instability)\n",
        "    \"steps\": 1000,          # ‚Üê Adjust based on your needs\n",
        "    \"compile\": False,       # PyTorch compilation (experimental)\n",
        "    \"dataset\": \"c4\"         # ‚Üê UPDATE THIS to your dataset\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(training_config)))\n",
        "# Write the training configuration to the config file\n",
        "cfg.training = OmegaConf.create(training_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallelism (FSDP)\n",
        "\n",
        "**You usually don't need to change this!**\n",
        "\n",
        "FSDP (Fully Sharded Data Parallel) automatically distributes your model across GPUs:\n",
        "- `-1` means \"use all available GPUs\" (recommended)\n",
        "- Each GPU holds only a portion of the model (e.g., 1/8th with 8 GPUs)\n",
        "- Enables training models that don't fit on a single GPU\n",
        "\n",
        "**For more details:** https://github.com/pytorch/torchtitan/tree/main/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Usually don't need to change these defaults\n",
        "parallelism_config = {\n",
        "    \"data_parallel_replicate_degree\": 1,\n",
        "    \"data_parallel_shard_degree\": -1,  # -1 = use all GPUs for FSDP\n",
        "    \"tensor_parallel_degree\": 1,\n",
        "    \"pipeline_parallel_degree\": 1,\n",
        "    \"context_parallel_degree\": 1,\n",
        "    \"expert_parallel_degree\": 1,\n",
        "    \"disable_loss_parallel\": False\n",
        "}\n",
        "\n",
        "print(\"Parallelism Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(parallelism_config)))\n",
        "\n",
        "# Write the parallelism configuration to the config file\n",
        "cfg.parallelism = OmegaConf.create(parallelism_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpointing\n",
        "\n",
        "**Critical settings to update:**\n",
        "\n",
        "Checkpoints save your training progress so you can resume if interrupted.\n",
        "\n",
        "- **folder**: Where to save checkpoints\n",
        "  - ‚ö†Ô∏è CHANGE THIS to your checkpoint directory\n",
        "  \n",
        "- **interval**: How often to save (in steps)\n",
        "  - 500 = save every 500 steps\n",
        "  - Lower = more frequent saves, but uses more disk space\n",
        "  \n",
        "- **initial_load_path**: Starting model weights\n",
        "  - Usually same as `hf_assets_path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è UPDATE these paths!\n",
        "checkpoint_config = {\n",
        "    \"enable\": True,\n",
        "    \"folder\": \"/path/to/your/checkpoint_folder\",  # ‚Üê UPDATE THIS\n",
        "    \"initial_load_path\": \"/path/to/your/model\",  # ‚Üê UPDATE THIS\n",
        "    \"initial_load_in_hf\": True,\n",
        "    \"last_save_in_hf\": True,\n",
        "    \"interval\": 500,           # Save every 500 steps\n",
        "    \"async_mode\": \"disabled\"\n",
        "}\n",
        "\n",
        "# Activation checkpointing (memory optimization)\n",
        "# Keep defaults unless running out of GPU memory\n",
        "activation_checkpoint_config = {\n",
        "    \"mode\": \"selective\",\n",
        "    \"selective_ac_option\": \"op\"\n",
        "}\n",
        "\n",
        "print(\"Checkpoint Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(checkpoint_config)))\n",
        "print(\"\\nActivation Checkpoint Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(activation_checkpoint_config)))\n",
        "\n",
        "# Write the checkpoint configuration to the config file\n",
        "cfg.checkpoint = OmegaConf.create(checkpoint_config)\n",
        "cfg.activation_checkpoint = OmegaConf.create(activation_checkpoint_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Complete Configuration\n",
        "\n",
        "Let's see the final configuration with all your overrides applied!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the final configuration\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(cfg)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Communication Settings\n",
        "\n",
        "**(Advanced - usually don't need to change)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the SFT\n",
        "\n",
        "We can simply run the experiment now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await run_actor(TrainerActor, cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîß Troubleshooting\n",
        "\n",
        "## Common Issues and Solutions\n",
        "\n",
        "### Out of Memory (OOM) Errors\n",
        "\n",
        "**Symptoms:** CUDA out of memory, process killed\n",
        "\n",
        "**Solutions:**\n",
        "1. Reduce `local_batch_size` (try 1 if using 2)\n",
        "2. Reduce `seq_len` (try 1024 instead of 2048)\n",
        "3. Enable more aggressive activation checkpointing:\n",
        "   ```python\n",
        "   activation_checkpoint_config = {\n",
        "       \"mode\": \"full\",  # More aggressive than \"selective\"\n",
        "       \"selective_ac_option\": \"op\"\n",
        "   }\n",
        "   ```\n",
        "4. Use more GPUs with FSDP to distribute the model\n",
        "\n",
        "---\n",
        "\n",
        "### Training is Slow\n",
        "\n",
        "**Solutions:**\n",
        "1. Increase `local_batch_size` if you have GPU memory headroom\n",
        "2. Use more GPUs (increase `procs`)\n",
        "3. Check if data loading is the bottleneck (add more dataloader workers)\n",
        "4. Reduce `seq_len` if you don't need long sequences\n",
        "\n",
        "---\n",
        "\n",
        "### Loss is NaN or Not Decreasing\n",
        "\n",
        "**Symptoms:** Training loss shows NaN or stays flat\n",
        "\n",
        "**Solutions:**\n",
        "1. **Lower learning rate** ‚Üí Try `lr: 5e-6` instead of `1e-5`\n",
        "2. **Check your data** ‚Üí Ensure labels are correct\n",
        "3. **Increase warmup** ‚Üí Try `warmup_steps: 500`\n",
        "4. **Gradient clipping** ‚Üí Ensure `max_norm: 1.0` is set\n",
        "\n",
        "---\n",
        "\n",
        "### Checkpoint Loading Fails\n",
        "\n",
        "**Symptoms:** Error loading from checkpoint folder\n",
        "\n",
        "**Solutions:**\n",
        "1. Check that `initial_load_path` exists and is accessible\n",
        "2. Verify HuggingFace format matches: `initial_load_in_hf: True`\n",
        "3. Ensure checkpoint folder has write permissions\n",
        "4. For resuming training, make sure the checkpoint folder contains valid checkpoints\n",
        "\n",
        "---\n",
        "\n",
        "### Multi-Node Training Issues\n",
        "\n",
        "**Symptoms:** Processes hang or timeout\n",
        "\n",
        "**Solutions:**\n",
        "1. Verify all nodes can communicate (check firewall rules)\n",
        "2. Ensure NCCL environment variables are set correctly\n",
        "3. Check that all nodes have the same code and dependencies\n",
        "4. Verify GPU topology with `nvidia-smi topo -m`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Appendix: Understanding Forge and Monarch\n",
        "\n",
        "## What is Forge?\n",
        "\n",
        "**Forge** is a distributed training framework built on PyTorch that simplifies multi-GPU and multi-node training. It abstracts away the complexity of distributed computing, letting you focus on your model and data.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Monarch?\n",
        "\n",
        "**Monarch** is PyTorch's distributed actor framework that powers Forge. Think of it as the \"engine\" under the hood.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "**Actors**: Encapsulated processes that manage distributed computation\n",
        "- `TrainerActor` in Forge is a Monarch actor that coordinates training\n",
        "- Each actor can spawn multiple processes (one per GPU)\n",
        "- Actors communicate via remote procedure calls (RPC)\n",
        "\n",
        "**Lifecycle**: Monarch actors follow a structured pattern\n",
        "```\n",
        "spawn() ‚Üí setup() ‚Üí run() ‚Üí cleanup()\n",
        "```\n",
        "\n",
        "**Why This Matters:**\n",
        "- Automatic process management across GPUs/nodes\n",
        "- Built-in fault tolerance with checkpointing\n",
        "- Clean resource management\n",
        "\n",
        "---\n",
        "\n",
        "## How `run_actor()` Works\n",
        "\n",
        "When you call `await run_actor(TrainerActor, cfg)`, here's what happens:\n",
        "\n",
        "### 1. **Spawn** üé≠\n",
        "```\n",
        "Monarch creates N processes (based on procs config)\n",
        "Each process gets assigned to a GPU\n",
        "Distributed communication (NCCL) initialized\n",
        "```\n",
        "\n",
        "### 2. **Setup** üîß  \n",
        "```\n",
        "Each process loads its portion of the model (FSDP sharding)\n",
        "Dataloaders created with different random seeds\n",
        "Checkpoint restored (if resuming training)\n",
        "```\n",
        "\n",
        "### 3. **Train** üèÉ\n",
        "```\n",
        "FOR each training step:\n",
        "    ‚Üí Get batch from dataloader\n",
        "    ‚Üí Forward pass (compute loss)\n",
        "    ‚Üí Backward pass (compute gradients)\n",
        "    ‚Üí FSDP automatically syncs gradients across GPUs\n",
        "    ‚Üí Optimizer updates weights\n",
        "    ‚Üí Save checkpoint periodically\n",
        "```\n",
        "\n",
        "### 4. **Cleanup** üßπ\n",
        "```\n",
        "Save final checkpoint\n",
        "Release GPU memory\n",
        "Terminate all processes cleanly\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## FSDP (Fully Sharded Data Parallel)\n",
        "\n",
        "FSDP is the secret sauce that makes training large models possible:\n",
        "\n",
        "**Without FSDP:**\n",
        "- Each GPU holds the full model ‚Üí Limited by single GPU memory\n",
        "- 8B model = ~16GB ‚Üí Won't fit on most GPUs\n",
        "\n",
        "**With FSDP:**\n",
        "- Model is sharded across GPUs ‚Üí Each GPU holds 1/N of the model\n",
        "- 8B model on 8 GPUs = ~2GB per GPU ‚Üí Fits easily!\n",
        "- Gradients automatically synchronized during backward pass\n",
        "\n",
        "**Configuration:**\n",
        "```python\n",
        "\"data_parallel_shard_degree\": -1  # -1 = use all GPUs\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Architecture?\n",
        "\n",
        "Traditional distributed training requires managing:\n",
        "- Process spawning and synchronization\n",
        "- GPU assignments and topology\n",
        "- Checkpoint coordination\n",
        "- Fault recovery\n",
        "- Communication primitives (NCCL, Gloo)\n",
        "\n",
        "**Forge + Monarch handles all of this automatically!**\n",
        "\n",
        "You just provide:\n",
        "- Model config\n",
        "- Training hyperparameters  \n",
        "- Number of GPUs\n",
        "\n",
        "Everything else is automatic.\n",
        "\n",
        "---\n",
        "\n",
        "## Learn More\n",
        "\n",
        "- **Forge docs**: https://github.com/meta-pytorch/torchforge/tree/main/docs\n",
        "- **Monarch docs**: https://github.com/meta-pytorch/monarch/tree/main/docs\n",
        "- **FSDP tutorial**: https://github.com/pytorch/torchtitan/tree/main/docs\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "924c63b2-fa48-4468-a04b-437f8bd23456",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "forge (conda)",
      "language": "python",
      "name": "conda_forge"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
